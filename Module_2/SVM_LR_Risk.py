import numpy as np
from sklearn import svm
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt


def error_calc(alpha,n,d,beta):
    theta0=np.ones(d)
    
    # Write a code that generates a synthetic dataset X,Y
    # X has n rows (number of data points) and d columns (number of features)
    # and Y is a 1-d array with n elements each being -1 or 1
    # each element of X is standard Gaussian
    # for the i element of Y is randomly generated by the
    # description in the pdf file and parameter alpha

    # run svm.SVC with parameters (C=1/alpha,kernel='linear')
    # also run Logisticregression with parameter (C=1/alpha)
    # See: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
    # See: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html

    # read out the coefficients of the trained models
    # calculate SQUARE L2 norm of the error between the coefficients of each model
    # and theta0, call the error_LR and error_SVM

    return error_LR, error_SVM

def avg_risk_calc(reg,n,d,alpha):
    # write a for loop that calculates the error for 1000 times
    # Although the parametrs reg, n,d,alpha are the same, each error is random
    # because the dataset is generated again
    # Calculate the average errors of 1000 runs 
    # You get MSE_LR and MSE_SVM
    return MSE_LR, MSE_SVM

# Now using avg_risk_calc function plot the risk function of alpha
# for different values n,d, reg 

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Module 7\n",
    "\n",
    "Group: 12<br>\n",
    "Darko Petrov<br>\n",
    "Alexander Lendon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.1\n",
    "### 1.\n",
    "#### a)\n",
    "In a consensus optimisation if the optimal value is reached it means that the vectors at each node are identical. An optimal value of zero means that the vectors $x_i$ at each node $v_i$ are equal because the objective value represents the distance between vectors at each pair of nodes.\n",
    "the set of all optimal solutions is given by:\n",
    "$$\n",
    "\\{(x_1,x_2,\\cdots,x_n) \\in (R^m)^n:x_1=x_2=\\cdots=x_n\\}\n",
    "$$",
    "### Q2\n",
    "#### (a) \n",
    "\n",
    "The gradient of the equation (4) can be gotten by taking the derivative with respect to $\\theta_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_i} (\\sum_{i}f_i(\\theta_i) + \\frac{\\rho}{2} \\sum_{i,j} a_{ij}\\| \\theta_i - \\theta_j \\|^2_2) = \\nabla f_i(\\theta_i) + \\rho \\sum_{i,j} a_{ij} (\\theta_i - \\theta_j)\n",
    "$$\n",
    "\n",
    "Now using the gradient we can write the iteration of GD with:\n",
    "\n",
    "$$\n",
    "\\theta_i^{k+1} = \\theta_i^k - \\alpha (\\nabla f_i(\\theta_i) + \\rho \\sum_{i,j} a_{ij} (\\theta_i - \\theta_j))\n",
    "$$\n",
    "\n",
    "Were $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "The original objective function is:\n",
    "\n",
    "$$\\sum_{i,j} a_{ij} \\|x_i - x_j\\|^2_2$$\n",
    "\n",
    "Expanding the squared norm, we get:\n",
    "\n",
    "$$\\sum_{i,j} a_{ij} (x_i^T x_i - 2x_i^T x_j + x_j^T x_j)$$\n",
    "\n",
    "Rearranging the terms, we get:\n",
    "\n",
    "$$\\sum_{i,j} a_{ij} x_i^T x_i - \\sum_{i,j} a_{ij} x_i^T x_j + \\sum_{i,j} a_{ij} x_j^T x_j$$\n",
    "\n",
    "This can be written as:\n",
    "\n",
    "$$\\sum_{i,j} L_{ij} x_i^T x_j$$\n",
    "\n",
    "The Laplacian matrix $L = (L_{ij})$ can be expressed as a degree matrix $D$ and the adjacency matrix $A$ where $L = D - A$. In our example we have a weighted graph. In this case the degree of a node is the sum of the weights of its edges, and the adjacency matrix uses the weights of the edges.\n",
    "\n",
    "For an undirected weighted graph, the elements of the Laplacian matrix $L$ are defined as follows:\n",
    "\n",
    "- For $i \\neq j$, $L_{ij} = -a_{ij}$ if there is an edge between nodes $v_i$ and $v_j$, and $L_{ij} = 0$ otherwise.\n",
    "- For $i = j$, $L_{ii} = \\sum_{j \\neq i} a_{ij}$, which is the sum of the weights of all edges connected to node $v_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "Defining the objective function $f(X)$:\n",
    "\n",
    "$$f(X) = \\sum_{i,j} L_{ij} x_i^T x_j$$\n",
    "\n",
    "where $X = [x_1, x_2, ..., x_n]^T$ is the matrix of vectors at each node, and $L_{ij}$ are the elements of the Laplacian matrix we defined previously.\n",
    "\n",
    "Taking the gradient of this function with respect to $x_k$,  \n",
    "$$\\frac{\\partial}{\\partial x_k}x_i^T x_j = \\begin{cases} x_i & \\text{if }k = j \\\\ x_j & \\text{if }k = i \\end{cases} $$\n",
    "\n",
    "So the gradient of $f(x_i)$ can be written as:$\n",
    "$$\\nabla f(x_i) = 2 \\sum_{j} L_{ij} x_j$$\n",
    "\n",
    "Now, a step of the gradient descent algorithm updates each vector $x_i$ as follows:\n",
    "\n",
    "$$x_i^{(t+1)} = x_i^{(t)} - \\mu \\nabla f(x_i^{(t)})$$\n",
    "\n",
    "where $\\mu$ is the step size. This update is performed for all $i = 1, 2, ..., n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)\n",
    "The gradient descent update rule was defined in the previous answer as:\n",
    "$$x_i^{(t+1)} = x_i^{(t)} - \\mu \\nabla f(x_i^{(t)})$$\n",
    "\n",
    "If we substitute in the expression for the gradient also found in the previous answer:\n",
    "$$\\nabla f(x_i) = 2 \\sum_{j} L_{ij} x_i$$\n",
    "the gradient descent can be written as:\n",
    "$$x_i^{(t+1)} = x_i^{(t)} - 2\\mu \\sum_{j} L_{ij} x_i^{(t)}$$\n",
    "We can now replace $L_{ij}$ as we found in part b) that:\n",
    "$$\n",
    "L_{ij} = -a_{ij} \\quad \\text{if} \\quad j\\neq i \\\\ \\\\\n",
    "L_{jj} = \\sum_{j\\neq i} a_{ij} \\quad \\text{otherwise}\n",
    "$$\n",
    "Resulting in the following expression for the gradient descent:\n",
    "$$x_i^{(t+1)} = x_i^{(t)} + 2\\mu \\sum_{j\\neq i} a_{ij}(x_i^{(t)} - x_j^{(t)})$$\n",
    "A gossip protocol update rule can be written as:\n",
    "$$\n",
    "x_i^{(t+1)}=\\sum_j w_{ij}x_j^{(t)}\n",
    "$$\n",
    "Rewriting the expression for gradient descent as this gossip protocol update rule would mean that:\n",
    "$$\n",
    "w_{ij} \n",
    "\\begin{cases}\n",
    "    \\delta_{ij} + 2\\mu a_{ij} & i\\neq j \\\\\n",
    "    1 - 2\\mu\\sum_{j\\neq i}a_{ij} & i=j\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\delta_{ij}\\begin{cases} 0 & i \\neq j \\\\ 1 & i=j \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e)\n",
    "Starting with the gossip protocol update rule and summing over all $i$:\n",
    "$$\n",
    "\\sum_{i=1}^{n}x_i^{(t+1)}=\\sum_{i=1}^{n}\\sum_j \\left(w_{ij}x_j^{(t)}\\right) + w_{ii}x_i^{(t)}\n",
    "$$\n",
    "Substitute in the expression for $\\sum_j w_{ij} = \\sum_{j\\neq i}(w_{ij})+w_{ii}$:\n",
    "$$\n",
    "\\sum_{i=1}^{n}x_i^{(t+1)}=\\sum_{i=1}^{n}\\sum_j\\left(\n",
    "1 - 2\\mu\\sum_{j\\neq i}a_{ij} +\n",
    "\\sum_j 2\\mu a_{ij}\n",
    "\\right)x_j^{(t)}\n",
    "$$\n",
    "Giving us:\n",
    "$$\n",
    "\\sum_{i=1}^{n}x_i^{(t+1)}=\\sum_{i=1}^{n}x_j^{(t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

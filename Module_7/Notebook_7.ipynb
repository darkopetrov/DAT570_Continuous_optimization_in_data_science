{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Module 7\n",
    "\n",
    "Group: 12<br>\n",
    "Darko Petrov<br>\n",
    "Alexander Lendon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.1\n",
    "### 1.\n",
    "#### a)\n",
    "In a consensus optimisation if the optimal value is reached it means that the vectors at each node are identical. An optimal value of zero means that the vectors $x_i$ at each node $v_i$ are equal because the objective value represents the distance between vectors at each pair of nodes.\n",
    "the set of all optimal solutions is given by:\n",
    "$$\n",
    "\\{(x_1,x_2,\\cdots,x_n) \\in (R^m)^n:x_1=x_2=\\cdots=x_n\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "The original objective function is:\n",
    "\n",
    "$$\\sum_{i,j} a_{ij} \\|x_i - x_j\\|^2_2$$\n",
    "\n",
    "Expanding the squared norm, we get:\n",
    "\n",
    "$$\\sum_{i,j} a_{ij} (x_i^T x_i - 2x_i^T x_j + x_j^T x_j)$$\n",
    "\n",
    "Rearranging the terms, we get:\n",
    "\n",
    "$$\\sum_{i,j} a_{ij} x_i^T x_i - \\sum_{i,j} a_{ij} x_i^T x_j + \\sum_{i,j} a_{ij} x_j^T x_j$$\n",
    "\n",
    "This can be written as:\n",
    "\n",
    "$$\\sum_{i,j} L_{ij} x_i^T x_j$$\n",
    "\n",
    "The Laplacian matrix $L = (L_{ij})$ can be expressed as a degree matrix $D$ and the adjacency matrix $A$ where $L = D - A$. In our example we have a weighted graph. In this case the degree of a node is the sum of the weights of its edges, and the adjacency matrix uses the weights of the edges.\n",
    "\n",
    "For an undirected weighted graph, the elements of the Laplacian matrix $L$ are defined as follows:\n",
    "\n",
    "- For $i \\neq j$, $L_{ij} = -a_{ij}$ if there is an edge between nodes $v_i$ and $v_j$, and $L_{ij} = 0$ otherwise.\n",
    "- For $i = j$, $L_{ii} = \\sum_{k \\neq i} a_{ik}$, which is the sum of the weights of all edges connected to node $v_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "Defining the objective function $f(X)$:\n",
    "\n",
    "$$f(X) = \\sum_{i,j} L_{ij} x_i^T x_j$$\n",
    "\n",
    "where $X = [x_1, x_2, ..., x_n]^T$ is the matrix of vectors at each node, and $L_{ij}$ are the elements of the Laplacian matrix we defined previously.\n",
    "\n",
    "Taking the gradient of this function with respect to $x_k$,  \n",
    "$$\\frac{\\partial}{\\partial x_k}x_i^T x_j = \\begin{cases} x_i & \\text{if }k = j \\\\ x_j & \\text{if }k = i \\end{cases} $$\n",
    "\n",
    "$$\\nabla f(x_k) = 2 \\sum_{i} L_{ki} x_i$$\n",
    "\n",
    "Now, a step of the gradient descent algorithm updates each vector $x_k$ as follows:\n",
    "\n",
    "$$x_k^{(t+1)} = x_k^{(t)} - \\mu \\nabla f(x_k^{(t)})$$\n",
    "\n",
    "where $\\mu$ is the step size. This update is performed for all $k = 1, 2, ..., n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
